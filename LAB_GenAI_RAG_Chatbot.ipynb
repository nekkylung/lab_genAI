{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsnCPbdkxYZd"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"color: #FF6347;\">Self-Guided Lab: Retrieval-Augmented Generation (RAGs)</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp4BQAVxYZj"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3FsdzRveTBrenMxM3VnbDMwaTJxN2NnZm50aGFibXk1NzNnY2Q0MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LR5ZBwZHv02lmpVoEU/giphy.gif\" alt=\"NLP Gif\" style=\"width: 300px; height: 150px; object-fit: cover; object-position: center;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizk6HCYxYZo"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Data Storage & Retrieval</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5UOI8ZxYZp"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">PyPDFLoader</h2>\n",
        "\n",
        "`PyPDFLoader` is a lightweight Python library designed to streamline the process of loading and parsing PDF documents for text processing tasks. It is particularly useful in Retrieval-Augmented Generation workflows where text extraction from PDFs is required.\n",
        "\n",
        "- **What Does PyPDFLoader Do?**\n",
        "  - Extracts text from PDF files, retaining formatting and layout.\n",
        "  - Simplifies the preprocessing of document-based datasets.\n",
        "  - Supports efficient and scalable loading of large PDF collections.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Compatible with popular NLP libraries and frameworks.\n",
        "  - Handles multi-page PDFs and embedded images (e.g., OCR-compatible setups).\n",
        "  - Provides flexible configurations for structured text extraction.\n",
        "\n",
        "- **Use Cases:**\n",
        "  - Preparing PDF documents for retrieval-based systems in RAGs.\n",
        "  - Automating the text extraction pipeline for document analysis.\n",
        "  - Creating datasets from academic papers, technical manuals, and reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_community in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (0.3.29)\n",
            "Requirement already satisfied: pypdf in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (6.0.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (0.4.25)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: termcolor in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (3.1.0)\n",
            "Requirement already satisfied: langchain_openai in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (0.3.32)\n",
            "Requirement already satisfied: langchain-huggingface in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (0.3.1)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (5.1.0)\n",
            "Requirement already satisfied: chromadb in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (1.0.20)\n",
            "Requirement already satisfied: langchain_chroma in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (0.2.5)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (0.11.0)\n",
            "Requirement already satisfied: openai in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (1.106.1)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain_openai) (0.3.75)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from tiktoken) (2025.8.29)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.25)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-huggingface) (0.22.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-huggingface) (0.34.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from sentence-transformers) (4.56.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: build>=1.0.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (0.17.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (14.1.0)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: pyproject_hooks in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: durationpy>=0.7 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.24.0)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nekky lung\\anaconda3\\envs\\ml\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain_community pypdf\n",
        "%pip install termcolor langchain_openai langchain-huggingface sentence-transformers chromadb langchain_chroma tiktoken openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6heKZkQUxYZr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRS44B2XxYZs",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Loading the Documents</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cuREtJRixYZt"
      },
      "outputs": [],
      "source": [
        "# # File path for the document\n",
        "\n",
        "# file_path = r\"../LAB/ai-for-everyone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz_8SOLxxYZt"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Documents into pages</h3>\n",
        "\n",
        "The `PyPDFLoader` library allows efficient loading and splitting of PDF documents into smaller, manageable parts for NLP tasks.\n",
        "\n",
        "This functionality is particularly useful in workflows requiring granular text processing, such as Retrieval-Augmented Generation (RAG).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_b5Z_45UxYZu",
        "outputId": "a600d69f-14fe-4492-f236-97261d6ff36c"
      },
      "outputs": [],
      "source": [
        "# # Load and split the document\n",
        "# loader = PyPDFLoader(file_path)\n",
        "# pages = loader.load_and_split()\n",
        "# len(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt50NRQaxYZv"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Pages into Chunks</h3>\n",
        "\n",
        "\n",
        "####  RecursiveCharacterTextSplitter in LangChain\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` is the **recommended splitter** in LangChain when you want to break down long documents into smaller, semantically meaningful chunks — especially useful in **RAG pipelines**, where clean context chunks lead to better LLM responses.\n",
        "\n",
        "####  Parameters\n",
        "\n",
        "| Parameter       | Description                                                                 |\n",
        "|-----------------|-----------------------------------------------------------------------------|\n",
        "| `chunk_size`    | The **maximum number of characters** allowed in a chunk (e.g., `1000`).     |\n",
        "| `chunk_overlap` | The number of **overlapping characters** between consecutive chunks (e.g., `200`). This helps preserve context continuity. |\n",
        "\n",
        "####  How it works\n",
        "`RecursiveCharacterTextSplitter` attempts to split the text **intelligently**, trying the following separators in order:\n",
        "1. Paragraphs (`\"\\n\\n\"`)\n",
        "2. Lines (`\"\\n\"`)\n",
        "3. Sentences or words (`\" \"`)\n",
        "4. Individual characters (as a last resort)\n",
        "\n",
        "This makes it ideal for handling **natural language documents**, such as PDFs, articles, or long reports, without breaking sentences or paragraphs in awkward ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=1000,\n",
        "#     chunk_overlap=200\n",
        "# )\n",
        "# chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Alternative: CharacterTextSplitter\n",
        "\n",
        "`CharacterTextSplitter` is a simpler splitter that breaks text into chunks based **purely on character count**, without trying to preserve any natural language structure.\n",
        "\n",
        "##### Example:\n",
        "```python\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "````\n",
        "\n",
        "This method is faster and more predictable but may split text in the middle of a sentence or paragraph, which can hurt performance in downstream tasks like retrieval or QA.\n",
        "\n",
        "---\n",
        "\n",
        "#### Comparison Table\n",
        "\n",
        "| Feature                        | RecursiveCharacterTextSplitter | CharacterTextSplitter     |\n",
        "| ------------------------------ | ------------------------------ | ------------------------- |\n",
        "| Structure-aware splitting      |  Yes                          |  No                      |\n",
        "| Preserves sentence/paragraphs  |  Yes                          |  No                      |\n",
        "| Risk of splitting mid-sentence |  Minimal                     |  High                   |\n",
        "| Ideal for RAG/document QA      |  Highly recommended           |  Only if structured text |\n",
        "| Performance speed              |  Slightly slower             |  Faster                  |\n",
        "\n",
        "---\n",
        "\n",
        "#### Recommendation\n",
        "\n",
        "Use `RecursiveCharacterTextSplitter` for most real-world document processing tasks, especially when building RAG pipelines or working with structured natural language content like PDFs or articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Choosing Chunk Size in RAG\n",
        "\n",
        "### Best Practices for Chunk Size in RAG\n",
        "\n",
        "| Factor                      | Recommendation                                                                                                                                                                                          |\n",
        "| ---------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **LLM context limit**       | Choose a chunk size that lets you retrieve multiple chunks **without exceeding the model’s token limit**. For example, GPT-4o supports 128k tokens, but with GPT-3.5 (16k) or GPT-4 (32k), keep it modest. |\n",
        "| **Chunk size (in characters)** | Typically: **500–1,000 characters** per chunk → ~75–200 tokens. This fits well for retrieval + prompt without context overflow.                                                                           |\n",
        "| **Chunk size (in tokens)**  | If using token-based splitter (e.g. `TokenTextSplitter`): aim for **100–300 tokens** per chunk.                                                                                                            |\n",
        "| **Chunk overlap**           | Use **overlap of 10–30%** (e.g., 100–300 characters or ~50 tokens) to preserve context across chunk boundaries and avoid cutting off important ideas mid-sentence.                                        |\n",
        "| **Document structure**      | Use **`RecursiveCharacterTextSplitter`** to preserve semantic boundaries (paragraphs, sentences) instead of arbitrary cuts.                                                                                |\n",
        "| **Task type**               | For **question answering**, smaller chunks (~500–800 chars) reduce noise.<br>For **summarization**, slightly larger chunks (~1000–1500) are OK.                                                          |\n",
        "| **Embedding model**         | Some models (e.g., `text-embedding-3-large`) can handle long input. But still, smaller chunks give **finer-grained retrieval**, which improves relevance.                                                  |\n",
        "| **Query type**              | If users ask **very specific questions**, small focused chunks are better. For broader queries, bigger chunks might help.                                                                                  |\n",
        "\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "| Use Case                 | Chunk Size      | Overlap |\n",
        "| ------------------------| --------------- | ------- |\n",
        "| Factual Q&A              | 500–800 chars   | 100–200 |\n",
        "| Summarization            | 1000–1500 chars | 200–300 |\n",
        "| Technical documents      | 400–700 chars   | 100–200 |\n",
        "| Long reports/books       | 800–1200 chars  | 200–300 |\n",
        "| Small LLMs (≤16k tokens) | ≤800 chars      | 100–200 |\n",
        "\n",
        "\n",
        "### Avoid\n",
        "\n",
        "- Chunks >2000 characters: risks context overflow.\n",
        "- No overlap: may lose key information between chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg15RjVPxYZw"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Embeddings</h2>\n",
        "\n",
        "Embeddings transform text into dense vector representations, capturing semantic meaning and contextual relationships. They are essential for efficient document retrieval and similarity analysis.\n",
        "\n",
        "- **What are OpenAI Embeddings?**\n",
        "  - Pre-trained embeddings like `text-embedding-3-large` generate high-quality vector representations for text.\n",
        "  - Encapsulate semantic relationships in the text, enabling robust NLP applications.\n",
        "\n",
        "- **Key Features of `text-embedding-3-large`:**\n",
        "  - Large-scale embedding model optimized for accuracy and versatility.\n",
        "  - Handles diverse NLP tasks, including retrieval, classification, and clustering.\n",
        "  - Ideal for applications with high-performance requirements.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Reduces the need for extensive custom training.\n",
        "  - Provides state-of-the-art performance in retrieval-augmented systems.\n",
        "  - Compatible with RAGs to create powerful context-aware models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L0xDxElwxYZw"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_WRIo3_0xYZx",
        "outputId": "78bfbbf3-9d25-4e31-bdbc-3e932e6bbfec"
      },
      "outputs": [],
      "source": [
        "# load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MNZfTng5xYZz",
        "outputId": "db1a7c85-ef9f-447e-92cd-9d097e959847"
      },
      "outputs": [],
      "source": [
        "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSA7RKvxYZz"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChromaDB</h2>\n",
        "\n",
        "ChromaDB is a versatile vector database designed for efficiently storing and retrieving embeddings. It integrates seamlessly with embedding models to enable high-performance similarity search and context-based retrieval.\n",
        "\n",
        "### Workflow Overview:\n",
        "- **Step 1:** Generate embeddings using a pre-trained model (e.g., OpenAI's `text-embedding-3-large`).\n",
        "- **Step 2:** Store the embeddings in ChromaDB for efficient retrieval and similarity calculations.\n",
        "- **Step 3:** Use the stored embeddings to perform searches, matching, or context-based retrieval.\n",
        "\n",
        "### Key Features of ChromaDB:\n",
        "- **Scalability:** Handles large-scale datasets with optimized indexing and search capabilities.\n",
        "- **Speed:** Provides fast and accurate retrieval of embeddings for real-time applications.\n",
        "- **Integration:** Supports integration with popular frameworks and libraries for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "brKe6wUgxYZ0"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VkjHR-RkxYZ0",
        "outputId": "bc11bda9-f283-457a-f584-5a06b95c4dd9"
      },
      "outputs": [],
      "source": [
        "# db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_LAB\")\n",
        "# print(\"ChromaDB created with document embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27OdN1IVxYZ1"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Retrieving Documents</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice1: Write a user question that someone might ask about your book’s topic or content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XiLv-TfrxYZ1"
      },
      "outputs": [],
      "source": [
        "# user_question = \"How to increase accuracy in the prediction?\" # User question\n",
        "# retrieved_docs = db.similarity_search(user_question, k=10) # k is the number of documents to retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qgWsh50JxYZ1",
        "outputId": "c8640c5d-5955-471f-fdd2-37096f5f68c7"
      },
      "outputs": [],
      "source": [
        "# # Display top results\n",
        "# for i, doc in enumerate(retrieved_docs[:3]): # Display top 3 results\n",
        "#     print(f\"Document {i+1}:\\n{doc.page_content[36:1000]}\") # Display content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuGK8gL6xYZ1"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Preparing Content for GenAI</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2iB3lZqHxYZ2"
      },
      "outputs": [],
      "source": [
        "# def _get_document_prompt(docs):\n",
        "#     prompt = \"\\n\"\n",
        "#     for doc in docs:\n",
        "#         prompt += \"\\nContent:\\n\"\n",
        "#         prompt += doc.page_content + \"\\n\\n\"\n",
        "#     return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2okzmuADxYZ2",
        "outputId": "0aa6cdca-188d-40e0-f5b4-8888d3549ea4"
      },
      "outputs": [],
      "source": [
        "# # Generate a formatted context from the retrieved documents\n",
        "# formatted_context = _get_document_prompt(retrieved_docs)\n",
        "# print(\"Context formatted for GPT model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzIczQNTxYZ2"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChatBot Architecture</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice2: Write a prompt that is relevant and tailored to the content and style of your book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tqxVh9s3xYZ3",
        "outputId": "97cca95d-4ab3-44d8-a76c-5713aad387d8"
      },
      "outputs": [],
      "source": [
        "# prompt = f\"\"\"\n",
        "# ## SYSTEM ROLE\n",
        "# You are a knowledgeable and factual chatbot designed to assist with technical questions about **AI**, specifically focusing on **Accuracy**.\n",
        "# Your answers must be based exclusively on provided content from technical books provided.\n",
        "\n",
        "# ## USER QUESTION\n",
        "# The user has asked:\n",
        "# \"{user_question}\"\n",
        "\n",
        "# ## CONTEXT\n",
        "# Here is the relevant content from the technical books:\n",
        "# '''\n",
        "# {formatted_context}\n",
        "# '''\n",
        "\n",
        "# ## GUIDELINES\n",
        "# 1. **Accuracy**:\n",
        "#    - Only use the content in the `CONTEXT` section to answer.\n",
        "#    - If the answer cannot be found, explicitly state: \"The provided context does not contain this information.\"\n",
        "#    - Start explain machine learning and then prediction in bulletpoints (charts, graphs, background and other aspects to consider)\n",
        "#    - Follow by differential diagnosis\n",
        "#    - Lastly explain the values for performance interpretation.\n",
        "\n",
        "# 2. **Transparency**:\n",
        "#    - Reference the book's name and page numbers when providing information.\n",
        "#    - Do not speculate or provide opinions.\n",
        "\n",
        "# 3. **Clarity**:\n",
        "#    - Use simple, professional, and concise language.\n",
        "#    - Format your response in Markdown for readability.\n",
        "\n",
        "# ## TASK\n",
        "# 1. Answer the user's question **directly** if possible.\n",
        "# 2. Point the user to relevant parts of the documentation.\n",
        "# 3. Provide the response in the following format:\n",
        "\n",
        "# ## RESPONSE FORMAT\n",
        "# '''\n",
        "# # [Brief Title of the Answer]\n",
        "# [Answer in simple, clear text.]\n",
        "\n",
        "# **Source**:\n",
        "# • [Book Title], Page(s): [...]\n",
        "# '''\n",
        "# \"\"\"\n",
        "# print(\"Prompt constructed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0mjkQJ_ZxYZ3"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice3: Tune parameters like temperature, and penalties to control how creative, focused, or varied the model's responses are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ylypRWRlxYZ4"
      },
      "outputs": [],
      "source": [
        "# # Set up GPT client and parameters\n",
        "# client = openai.OpenAI()\n",
        "# model_params = {\n",
        "#     'model': 'gpt-4o',\n",
        "#     'temperature': 0.9,  # Increase creativity\n",
        "#     'max_tokens': 4000,  # Allow for longer responses\n",
        "#     'top_p': 0.9,        # Use nucleus sampling\n",
        "#     'frequency_penalty': 0.5,  # Reduce repetition\n",
        "#     'presence_penalty': 0.6    # Encourage new topics\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8e942xDxYZ4"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Response</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4eXZO4pIxYZ4"
      },
      "outputs": [],
      "source": [
        "# messages = [{'role': 'user', 'content': prompt}]\n",
        "# completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wLPAcchBxYZ5",
        "outputId": "976c7800-16ed-41fe-c4cf-58f60d3230d2"
      },
      "outputs": [],
      "source": [
        "# answer = completion.choices[0].message.content\n",
        "# print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXVNXPwLxYaT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png\" alt=\"NLP Gif\" style=\"width: 500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldybhlqKxYaT"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Cosine Similarity</h2>\n",
        "\n",
        "**Cosine similarity** is a metric used to measure the alignment or similarity between two vectors, calculated as the cosine of the angle between them. It is the **most common metric used in RAG pipelines** for vector retrieval.. It provides a scale from -1 to 1:\n",
        "\n",
        "- **-1**: Vectors are completely opposite.\n",
        "- **0**: Vectors are orthogonal (uncorrelated or unrelated).\n",
        "- **1**: Vectors are identical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1I1TNhxYaT"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\" alt=\"NLP Gif\" style=\"width: 700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEMdNgQxYaU"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Keyword Highlighting</h2>\n",
        "\n",
        "Highlighting important keywords helps users quickly understand the relevance of the retrieved text to their query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nCXL9Cz1xYaV"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwDyofY0xYaV"
      },
      "source": [
        "The `highlight_keywords` function is designed to highlight specific keywords within a given text. It replaces each keyword in the text with a highlighted version using the `colored` function from the `termcolor` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9y3E0YWExYaV"
      },
      "outputs": [],
      "source": [
        "# def highlight_keywords(text, keywords):\n",
        "#     for keyword in keywords:\n",
        "#         text = text.replace(keyword, colored(keyword, 'green', attrs=['bold']))\n",
        "#     return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice4: add your keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "i7SkWPpnxYaW",
        "outputId": "28e82563-edba-4b41-acad-ec27e5ba134f"
      },
      "outputs": [],
      "source": [
        "# query_keywords = [\"statistic\",\"chart\",\"algorithm\"] # add your keywords\n",
        "# for i, doc in enumerate(retrieved_docs[:1]):\n",
        "#     snippet = doc.page_content[:200]\n",
        "#     highlighted = highlight_keywords(snippet, query_keywords)\n",
        "#     print(f\"Snippet {i+1}:\\n{highlighted}\\n{'-'*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhV_Jf_LxYaX"
      },
      "source": [
        "1. `query_keywords` is a list of keywords to be highlighted.\n",
        "2. The loop iterates over the first document in retrieved_docs.\n",
        "3. For each document, a snippet of the first 200 characters is extracted.\n",
        "4. The highlight_keywords function is called to highlight the keywords in the snippet.\n",
        "5. The highlighted snippet is printed along with a separator line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRKysAvxYaX"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Bonus</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj25lCybxYaX"
      },
      "source": [
        "**Try loading one of your own PDF books and go through the steps again to explore how the pipeline works with your content**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1119"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# File path for the document\n",
        "file_path = r\"C:\\Users\\Nekky Lung\\Desktop\\IronHack AI Course\\2_FT_July2025\\week7\\day5\\LAB\\H.M._Raghunath_Hydrology_Principles_Analysis.pdf\"\n",
        "\n",
        "# Load and split the document\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load_and_split()\n",
        "len(pages)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB created with document embeddings.\n",
            "Document 1:\n",
            "tive frequency curve\n",
            "Annual No. of Cumulative Probability\n",
            "flood peak occurrences occurrences =×FHG IKJ\n",
            "CF\n",
            "f 100Σ\n",
            "%\n",
            "C.I. or or\n",
            "(1000 cumec) frequency, f frequency, CF\n",
            "0-2* 0 87 100\n",
            "2-4* 17 87 100\n",
            "4-6 27 70 80.5\n",
            "6-8 18 43 49.5\n",
            "8-10 18 25 28.8\n",
            "10-12 3 7 8.05\n",
            "12-14 0 4 4.6\n",
            "14-16 2 4 4.6\n",
            "16-18 1 2 2.3\n",
            "18-20 1 1 1.15\n",
            "Σf = 87\n",
            "*0-<2.\n",
            "2- <4, and like that.\n",
            "(a) Partial duration series. There are 175 flood exceedances (above Qb) during 87\n",
            "years. Average number of exceedances per year.\n",
            "λ = 175\n",
            "87  = 2.01\n",
            "73. 1957 4548 3.6579\n",
            "74. 1958 4056 3.6081\n",
            "75. 1959 4493 3.6525\n",
            "76. 1960 3884 3.5893\n",
            "77. 1961 4855 3.6861\n",
            "78. 1962 5760 3.7604\n",
            "79. 1963 9192 3.9634\n",
            "80. 1964 3024 3.4806\n",
            "81. 1965 2509 3.3994\n",
            "82. 1966 4741 4.6759\n",
            "83. 1967 5919 3.7725\n",
            "84. 1968 3798 3.5795\n",
            "85. 1969 4546 3.6577\n",
            "86. 1970 3842 3.5845\n",
            "87. 1971 4542 3.6573\n",
            "Document 2:\n",
            "interval of flood (T-yr)\n",
            "5 10 50 100 200\n",
            "1 20 10 2 1 0.5\n",
            "56 7 41 10 5 2\n",
            "10 89 65 18 10 5\n",
            "25 99.6 93 40 22 12\n",
            "50 — 99.5 64 40 22\n",
            "100 — — 87 63 39\n",
            "200 — — 98 87 63\n",
            "500 — — — 99.3 92\n",
            "Partial duration curve method  Partial duration curves are plotted showing the flood\n",
            "discharges against their probable frequency of occurrence in 100 years and not against per-\n",
            "centage of time as in the annual flood series.\n",
            "Probable frequency = \n",
            "m\n",
            "y  × 100 ...(8.24)\n",
            "where m = order number or rank of the particular flood in the series of items selected and\n",
            "arranged in the descending order of magnitude\n",
            "y = total length of record in years\n",
            "The number of flood items selected need not be greater than the number of years in\n",
            "record to simplify the procedure. Gumbel paper should not be used for partial series, which\n",
            "usually plot better on semi-log paper.\n",
            "Frequency method may be used for drainage basins of any size. The reliability of the\n",
            "Document 3:\n",
            "TIMATION AND CONTROL 227\n",
            "assurance, while the 50-yr or 100-yr floods could be estimated only very approximately. The\n",
            "procedure in frequency analysis is ( i) compilation of flood peaks in the descending order of\n",
            "magnitude, (ii) computation of recurrence intervals by any one of the stochastic methods, and\n",
            "(iii) plotting ‘the flood peaks versus recurrence interval’ from which the flood (or rainfall) of a\n",
            "required recurrence interval can be read or extrapolated, or the recurrence interval for a given\n",
            "flood magnitude (or rainfall) can be read or extrapolated.\n",
            "The limitations of frquency methods are that the greatest floods are caused by an unu-\n",
            "sual meteorological combination resulting in unexpected peculiar storms. Hence, the probabil-\n",
            "ity studies alone are inadequate for predicting floods of very great magnitudes.\n",
            "Example 8.5 Flood frequency studies are made for the 30-year flood data (from 1939-1968) of\n"
          ]
        }
      ],
      "source": [
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_lesson\")\n",
        "print(\"ChromaDB created with document embeddings.\")\n",
        "\n",
        "user_question = \"How I calculate Flood Frequency?\" # User question\n",
        "retrieved_docs = db.similarity_search(user_question, k=10) # k is the number of documents to retrieve\n",
        "\n",
        "# Display top results\n",
        "for i, doc in enumerate(retrieved_docs[:3]): # Display top 3 results\n",
        "    print(f\"Document {i+1}:\\n{doc.page_content[36:1000]}\") # Display content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context formatted for GPT model.\n"
          ]
        }
      ],
      "source": [
        "def _get_document_prompt(docs):\n",
        "    prompt = \"\\n\"\n",
        "    for doc in docs:\n",
        "        prompt += \"\\nContent:\\n\"\n",
        "        prompt += doc.page_content + \"\\n\\n\"\n",
        "    return prompt\n",
        "\n",
        "# Generate a formatted context from the retrieved documents\n",
        "formatted_context = _get_document_prompt(retrieved_docs)\n",
        "print(\"Context formatted for GPT model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt constructed.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "## SYSTEM ROLE\n",
        "You are a knowledgeable and factual chatbot designed to assist with technical questions about **Hydrology**, specifically focusing on **flooding**.\n",
        "Your answers must be based exclusively on provided content from technical books provided.\n",
        "\n",
        "## USER QUESTION\n",
        "The user has asked:\n",
        "\"{user_question}\"\n",
        "\n",
        "## CONTEXT\n",
        "Here is the relevant content from the technical books:\n",
        "'''\n",
        "{formatted_context}\n",
        "'''\n",
        "\n",
        "## GUIDELINES\n",
        "1. **Accuracy**:\n",
        "   - Only use the content in the `CONTEXT` section to answer.\n",
        "   - If the answer cannot be found, explicitly state: \"The provided context does not contain this information.\"\n",
        "   - Start explain hydrology and then statistic in bulletpoints (calculation, values, background , graph and other aspects to consider)\n",
        "   - Follow by differential diagnosis\n",
        "   - Lastly explain the values for interpretation.\n",
        "\n",
        "2. **Transparency**:\n",
        "   - Reference the book's name and page numbers when providing information.\n",
        "   - Do not speculate or provide opinions.\n",
        "\n",
        "3. **Clarity**:\n",
        "   - Use simple, professional, and concise language.\n",
        "   - Format your response in Markdown for readability.\n",
        "\n",
        "## TASK\n",
        "1. Answer the user's question **directly** if possible.\n",
        "2. Point the user to relevant parts of the documentation.\n",
        "3. Provide the response in the following format:\n",
        "\n",
        "## RESPONSE FORMAT\n",
        "'''\n",
        "# [Brief Title of the Answer]\n",
        "[Answer in simple, clear text.]\n",
        "\n",
        "**Source**:\n",
        "• [Book Title], Page(s): [...]\n",
        "'''\n",
        "\"\"\"\n",
        "print(\"Prompt constructed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Set up GPT client and parameters\n",
        "client = openai.OpenAI()\n",
        "model_params = {\n",
        "    'model': 'gpt-4o',\n",
        "    'temperature': 0.7,  # Increase creativity\n",
        "    'max_tokens': 4000,  # Allow for longer responses\n",
        "    'top_p': 0.9,        # Use nucleus sampling\n",
        "    'frequency_penalty': 0.5,  # Reduce repetition\n",
        "    'presence_penalty': 0.6    # Encourage new topics\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'''\n",
            "# Calculating Flood Frequency\n",
            "\n",
            "To calculate flood frequency, you can use statistical methods to estimate how often floods of a certain magnitude are expected to occur. Here's a step-by-step guide based on the provided context:\n",
            "\n",
            "- **Hydrology Background**:\n",
            "  - Flood frequency analysis is used to predict the probability of flood events over time.\n",
            "  - It involves compiling and analyzing historical data of flood peaks.\n",
            "\n",
            "- **Statistical Calculation**:\n",
            "  - Arrange stream flow peaks in descending order by magnitude; this forms the basis for statistical analysis.\n",
            "  - Compute recurrence intervals using stochastic methods. The recurrence interval (T-year) indicates the average time between floods of a given size.\n",
            "  - Use formulas like Gumbel's or Weibull's method, which involve plotting on specialized paper (e.g., semi-log or log-log paper).\n",
            "\n",
            "- **Values and Graphs**:\n",
            "  - The cumulative frequency curve is computed with tables like Table 15.6, which lists annual flood occurrences and probabilities.\n",
            "  - Partial duration series can also be analyzed for more frequent but smaller flood events.\n",
            "\n",
            "- **Considerations**:\n",
            "  - Reliability depends on record length; longer records provide more reliable estimates.\n",
            "  - Extreme floods may result from unique meteorological conditions not predictable by statistical means alone.\n",
            "\n",
            "**Source**:\n",
            "• [Flood Estimation and Control], Pages: Various sections including pages: C-9\\N-HYDRO\\HYD8-1.PM5, HYD15-2.PM5\n",
            "'''\n"
          ]
        }
      ],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)\n",
        "\n",
        "answer = completion.choices[0].message.content\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
